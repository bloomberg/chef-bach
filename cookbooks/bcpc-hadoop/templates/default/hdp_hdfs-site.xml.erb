<?xml version="1.0"?>
<!--
################################################
#
#              Generated by Chef
#
################################################
-->
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration xmlns:xi="http://www.w3.org/2001/XInclude">
  <property>
    <name>dfs.replication</name>
    <value><%= node[:bcpc][:hadoop][:hdfs][:dfs_replication_factor] %></value>
  </property>

  <property>
    <name>dfs.namenode.audit.log.async</name>
    <value><%= node["bcpc"]["hadoop"]["hdfs"]["dfs"]["namenode"]["audit"]["log"]["async"]%></value>
  </property>

  <property>
    <name>dfs.datanode.balance.bandwidthPerSec</name>
    <value><%=node[:bcpc][:hadoop][:balancer][:bandwidth]%></value>
  </property>

  <property>
    <name>dfs.namenode.name.dir</name>
    <value><%=@mounts.map{ |d| "file:///disk/#{d}/dfs/nn" }.join(",")%></value>
  </property>

  <property>
    <name>dfs.nameservices</name>
    <value><%= node.chef_environment %></value>
  </property>

  <property>
    <name>dfs.ha.namenodes.<%= node.chef_environment %></name>
    <value><%=@nn_hosts.map{ |s| "namenode#{s[:node_number]}" }.join(",") %></value>
  </property>

  <property>
    <name>dfs.datanode.data.dir</name>
    <value><%=@mounts.map{ |d| "file:///disk/#{d}/dfs/dn" }.join(",")%></value>
  </property>

  <property>
    <name>dfs.datanode.failed.volumes.tolerated</name>
    <value><%= node[:bcpc][:hadoop][:hdfs][:failed_volumes_tolerated] %></value>
  </property>

  <% @nn_hosts.each do |h| %>	
  <property>
    <name>dfs.namenode.rpc-address.<%= "#{node.chef_environment}.namenode#{h[:node_number]}" %></name>
    <value><%=float_host(h[:hostname]) %>:<%= node[:bcpc][:hadoop][:namenode][:rpc][:port] %></value>
  </property>
  <% end %>

  <% @nn_hosts.each do |h| %>	
  <property>
    <name>dfs.namenode.http-address.<%= "#{node.chef_environment}.namenode#{h[:node_number]}" %></name>
    <value><%=float_host(h[:hostname]) %>:<%= node[:bcpc][:hadoop][:namenode][:http][:port] %></value>
  </property>
  <% end %>

  <% @nn_hosts.each do |h| %>	
  <property>
    <name>dfs.namenode.https-address.<%= "#{node.chef_environment}.namenode#{h[:node_number]}" %></name>
    <value><%=float_host(h[:hostname]) %>:<%= node[:bcpc][:hadoop][:namenode][:https][:port] %></value>
  </property>
  <% end %>

  <property>
    <name>dfs.client.failover.proxy.provider.<%= node.chef_environment %></name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>

  <% if node[:bcpc][:hadoop][:hdfs][:HA] == true then %>
  <%=  '<xi:include href="file:///etc/hadoop/conf/hdfs-site_HA.xml"/>' %>
  <% end %>

  <property>
    <name>dfs.webhdfs.enabled</name>
    <value><%= node["bcpc"]["hadoop"]["hdfs"]["dfs"]["webhdfs"]["enabled"]%></value>
  </property>

  <property>
    <name>dfs.client.read.shortcircuit</name>
    <value><%= node["bcpc"]["hadoop"]["hdfs"]["dfs"]["client"]["read"]["shortcircuit"]%></value>
  </property>

  <property>
    <name>dfs.domain.socket.path</name>
    <value><%= node["bcpc"]["hadoop"]["hdfs"]["dfs"]["domain"]["socket"]["path"]%></value>
  </property>

  <property>
    <name>dfs.client.file-block-storage-locations.timeout</name>
    <value><%= node["bcpc"]["hadoop"]["hdfs"]["dfs"]["client"]["file-block-storage-locations"]["timeout"]%></value>
  </property>

  <property>
    <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
    <value><%= node["bcpc"]["hadoop"]["hdfs"]["dfs"]["datanode"]["hdfs-blocks-metadata"]["enabled"]%></value>
  </property>

  <property>
    <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
    <value><%= node["bcpc"]["hadoop"]["hdfs"]["dfs"]["namenode"]["datanode"]["registration"]["ip-hostname-check"]%></value>
  </property>

  <property>
    <name>dfs.datanode.ipc.address</name>
    <value><%=node[:bcpc][:floating][:ip]%>:50020</value>
  </property>

  <property>
    <name>dfs.client.local.interfaces</name>
    <value><%= node['bcpc']['networks'][node['bcpc']['management']['subnet']]['floating']['interface'] %></value>
  </property>

  <property>
    <name>dfs.namenode.avoid.read.stale.datanode</name>
    <value><%= node["bcpc"]["hadoop"]["hdfs"]["dfs"]["namenode"]["avoid"]["read"]["stale"]["datanode"]%></value>
  </property>

  <property>
    <name>dfs.namenode.avoid.write.stale.datanode</name>
    <value><%= node["bcpc"]["hadoop"]["hdfs"]["dfs"]["namenode"]["avoid"]["write"]["stale"]["datanode"]%></value>
  </property>

  <property>
    <name>dfs.hosts.exclude</name>
    <value><%= node["bcpc"]["hadoop"]["hdfs"]["dfs"]["hosts"]["exclude"]%></value>
  </property>

  <property>
    <name>dfs.datanode.du.reserved</name>
    <!-- Reserve 1GB on each datanode -->
    <value><%= node["bcpc"]["hadoop"]["hdfs"]["dfs"]["datanode"]["du"]["reserved"]%></value>
  </property>

  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value><%="/disk/#{@mounts[0]}/dfs/jn"%></value>
  </property>

  <property>
    <name>dfs.blocksize</name>
    <value><%= node["bcpc"]["hadoop"]["hdfs"]["dfs_blocksize"] %></value>
  </property>

  <property>
    <name>dfs.datanode.max.transfer.threads</name>
    <value><%= node["bcpc"]["hadoop"]["datanode"]["max"]["xferthreads"] %></value>
  </property>

  <property>
    <name>dfs.namenode.handler.count</name>
    <value><%= node["bcpc"]["hadoop"]["namenode"]["handler"]["count"] %></value>
  </property>

  <% if node[:bcpc][:hadoop][:kerberos][:enable] == true then %>
  <property> 
    <name>dfs.permissions.enabled</name> 
    <value>true</value> 
    <description> 
      If "true", enables permission checking in HDFS. If "false", permission checking is turned
      off, but all other behavior is unchanged. Switching from one parameter value to the other does
      not change the mode, owner or group of files or directories.
    </description> 
  </property> 

  <property> 
    <name>dfs.block.access.token.enable</name> 
    <value>true</value> 
    <description> 
      If "true", access tokens are used to access datanodes. If "false", no access tokens are checked on
      accessing datanodes. 
    </description> 
  </property> 

  <property>
    <name>dfs.permissions.superusergroup</name>
    <value><%= node["bcpc"]["hadoop"]["hdfs"]["dfs"]["permissions"]["superusergroup"]%></value>
  </property>

  <property>
    <name>dfs.namenode.kerberos.principal</name>
    <value><%= node[:bcpc][:hadoop][:kerberos][:data][:namenode][:principal] %>/<%=node[:bcpc][:hadoop][:kerberos][:data][:namenode][:princhost] == "_HOST" ? "_HOST" : node[:bcpc][:hadoop][:kerberos][:data][:namenode][:princhost]%>@<%= node[:bcpc][:hadoop][:kerberos][:realm] %></value>
    <description> Kerberos principal name for the NameNode</description>
  </property>

  <property>
    <name>dfs.web.authentication.kerberos.principal</name>
    <value><%= node[:bcpc][:hadoop][:kerberos][:data][:spnego][:principal] %>/<%=node[:bcpc][:hadoop][:kerberos][:data][:spnego][:princhost] == "_HOST" ? "_HOST" : node[:bcpc][:hadoop][:kerberos][:data][:spnego][:princhost]%>@<%= node[:bcpc][:hadoop][:kerberos][:realm] %></value>
  </property>

  <property> 
    <name>dfs.web.authentication.kerberos.keytab</name> 
    <value><%= node[:bcpc][:hadoop][:kerberos][:keytab][:dir] %>/<%= node[:bcpc][:hadoop][:kerberos][:data][:namenode][:spnego_keytab] %></value> 
    <description>The Kerberos keytab file with the credentials for the HTTP Kerberos principal used by Hadoop-Auth in the HTTP endpoint. 
    </description> 
  </property> 

  <property> 
    <name>dfs.namenode.keytab.file</name> 
    <value><%= node[:bcpc][:hadoop][:kerberos][:keytab][:dir] %>/<%= node[:bcpc][:hadoop][:kerberos][:data][:namenode][:keytab] %></value> 
    <description>Combined keytab file containing the namenode service and host principals.</description> 
  </property> 

  <property> 
    <name>dfs.datanode.keytab.file</name> 
    <value><%= node[:bcpc][:hadoop][:kerberos][:keytab][:dir] %>/<%= node[:bcpc][:hadoop][:kerberos][:data][:datanode][:keytab] %></value> 
    <description>The filename of the keytab file for the DataNode.</description> 
  </property> 

  <property> 
    <name>dfs.cluster.administrators</name> 
    <value><%= node["bcpc"]["hadoop"]["hdfs"]["dfs"]["cluster"]["administrators"]%></value> 
    <description>ACL for who all can view the default servlets in the HDFS</description> 
  </property> 

  <property> 
    <name>dfs.namenode.kerberos.internal.spnego.principal</name> 
    <value>${dfs.web.authentication.kerberos.principal}</value> 
  </property> 

  <property> 
    <name>dfs.secondary.namenode.kerberos.internal.spnego.principal</name> 
    <value>${dfs.web.authentication.kerberos.principal}</value> 
  </property>

  <% if node.run_list.expand(node.chef_environment).recipes.include?("bcpc-hadoop::journalnode")  %>
  <property>
    <name>dfs.journalnode.rpc-address</name>
    <value><%=node[:bcpc][:floating][:ip]%>:8485</value>
  </property>

  <property>
    <name>dfs.journalnode.http-address</name>
    <value><%=node[:bcpc][:floating][:ip]%>:8480</value>
  </property>

  <property>
    <name>dfs.journalnode.https-address</name>
    <value><%=node[:bcpc][:floating][:ip]%>:8481</value>
  </property>

  <property>
    <name>dfs.journalnode.keytab.file</name>
    <value><%= node[:bcpc][:hadoop][:kerberos][:keytab][:dir] %>/<%= node[:bcpc][:hadoop][:kerberos][:data][:journalnode][:keytab] %></value> 
  </property>

  <property>
    <name>dfs.journalnode.kerberos.principal</name>
    <value><%= node[:bcpc][:hadoop][:kerberos][:data][:journalnode][:principal] %>/<%=node[:bcpc][:hadoop][:kerberos][:data][:journalnode][:princhost] == "_HOST" ? "_HOST" : node[:bcpc][:hadoop][:kerberos][:data][:journalnode][:princhost]%>@<%= node[:bcpc][:hadoop][:kerberos][:realm] %></value>
  </property>

  <property>
    <name>dfs.journalnode.kerberos.internal.spnego.principal</name>
    <value><%= node[:bcpc][:hadoop][:kerberos][:data][:spnego][:principal] %>/<%= node[:bcpc][:hadoop][:kerberos][:data][:spnego][:princhost] == "_HOST" ? float_host(node[:fqdn]) : node[:bcpc][:hadoop][:kerberos][:data][:spnego][:princhost] %>@<%= node[:bcpc][:hadoop][:kerberos][:realm] %></value>
  </property>
  <% end %>

  <% if node.run_list.expand(node.chef_environment).recipes.include?("bcpc-hadoop::datanode")  %>
  <property>
    <name>dfs.datanode.kerberos.principal</name>
    <value><%= node[:bcpc][:hadoop][:kerberos][:data][:datanode][:principal] %>/<%= node[:bcpc][:hadoop][:kerberos][:data][:datanode][:princhost] == "_HOST" ? float_host(node[:fqdn]) : node[:bcpc][:hadoop][:kerberos][:data][:datanode][:princhost] %>@<%= node[:bcpc][:hadoop][:kerberos][:realm] %></value>
  </property>

  <property>
    <name>dfs.datanode.address</name>
    <value><%= node[:bcpc][:floating][:ip] %>:1004</value>
  </property>

  <property>
    <name>dfs.datanode.http.address</name>
    <value><%= node[:bcpc][:floating][:ip] %>:1006</value>
  </property>
  <% else %>
  <property>
    <name>dfs.datanode.kerberos.principal</name>
    <value><%= node[:bcpc][:hadoop][:kerberos][:data][:datanode][:principal] %>/<%=node[:bcpc][:hadoop][:kerberos][:data][:datanode][:princhost] == "_HOST" ? "_HOST" : node[:bcpc][:hadoop][:kerberos][:data][:datanode][:princhost]%>@<%= node[:bcpc][:hadoop][:kerberos][:realm] %></value>
  </property>
  <%end%>
  <%end%>
</configuration>
